{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10632457,"sourceType":"datasetVersion","datasetId":6582787}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:48.268243Z","iopub.execute_input":"2025-02-09T16:46:48.268662Z","iopub.status.idle":"2025-02-09T16:46:48.273028Z","shell.execute_reply.started":"2025-02-09T16:46:48.268619Z","shell.execute_reply":"2025-02-09T16:46:48.272229Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/multimodal-sentiment/Multimodal_dataset_assignment3/labels.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:48.274025Z","iopub.execute_input":"2025-02-09T16:46:48.274275Z","iopub.status.idle":"2025-02-09T16:46:48.407269Z","shell.execute_reply.started":"2025-02-09T16:46:48.274249Z","shell.execute_reply":"2025-02-09T16:46:48.405991Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:48.409419Z","iopub.execute_input":"2025-02-09T16:46:48.409817Z","iopub.status.idle":"2025-02-09T16:46:48.443274Z","shell.execute_reply.started":"2025-02-09T16:46:48.409773Z","shell.execute_reply":"2025-02-09T16:46:48.441688Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0    image_name  \\\n0           0   image_1.jpg   \n1           1  image_2.jpeg   \n2           2   image_3.JPG   \n3           3   image_4.png   \n4           4   image_5.png   \n\n                                            text_ocr  \\\n0  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   \n1  The best of #10 YearChallenge! Completed in le...   \n2  Sam Thorne @Strippin ( Follow Follow Saw every...   \n3              10 Year Challenge - Sweet Dee Edition   \n4  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   \n\n                                      text_corrected      humour  \\\n0  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   hilarious   \n1  The best of #10 YearChallenge! Completed in le...   not_funny   \n2  Sam Thorne @Strippin ( Follow Follow Saw every...  very_funny   \n3              10 Year Challenge - Sweet Dee Edition  very_funny   \n4  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   hilarious   \n\n           sarcasm       offensive      motivational overall_sentiment  \n0          general   not_offensive  not_motivational     very_positive  \n1          general   not_offensive      motivational     very_positive  \n2    not_sarcastic   not_offensive  not_motivational          positive  \n3  twisted_meaning  very_offensive      motivational          positive  \n4     very_twisted  very_offensive  not_motivational           neutral  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>image_name</th>\n      <th>text_ocr</th>\n      <th>text_corrected</th>\n      <th>humour</th>\n      <th>sarcasm</th>\n      <th>offensive</th>\n      <th>motivational</th>\n      <th>overall_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>image_1.jpg</td>\n      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n      <td>hilarious</td>\n      <td>general</td>\n      <td>not_offensive</td>\n      <td>not_motivational</td>\n      <td>very_positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>image_2.jpeg</td>\n      <td>The best of #10 YearChallenge! Completed in le...</td>\n      <td>The best of #10 YearChallenge! Completed in le...</td>\n      <td>not_funny</td>\n      <td>general</td>\n      <td>not_offensive</td>\n      <td>motivational</td>\n      <td>very_positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>image_3.JPG</td>\n      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n      <td>very_funny</td>\n      <td>not_sarcastic</td>\n      <td>not_offensive</td>\n      <td>not_motivational</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>image_4.png</td>\n      <td>10 Year Challenge - Sweet Dee Edition</td>\n      <td>10 Year Challenge - Sweet Dee Edition</td>\n      <td>very_funny</td>\n      <td>twisted_meaning</td>\n      <td>very_offensive</td>\n      <td>motivational</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>image_5.png</td>\n      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n      <td>hilarious</td>\n      <td>very_twisted</td>\n      <td>very_offensive</td>\n      <td>not_motivational</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:48.444942Z","iopub.execute_input":"2025-02-09T16:46:48.445507Z","iopub.status.idle":"2025-02-09T16:46:48.460432Z","shell.execute_reply.started":"2025-02-09T16:46:48.445475Z","shell.execute_reply":"2025-02-09T16:46:48.459922Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Unnamed: 0             0\nimage_name             0\ntext_ocr             161\ntext_corrected         5\nhumour                 0\nsarcasm                0\noffensive              0\nmotivational           0\noverall_sentiment      0\ndtype: int64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:48.460957Z","iopub.execute_input":"2025-02-09T16:46:48.461255Z","iopub.status.idle":"2025-02-09T16:46:48.466649Z","shell.execute_reply.started":"2025-02-09T16:46:48.461225Z","shell.execute_reply":"2025-02-09T16:46:48.465569Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Index(['Unnamed: 0', 'image_name', 'text_ocr', 'text_corrected', 'humour',\n       'sarcasm', 'offensive', 'motivational', 'overall_sentiment'],\n      dtype='object')"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df = df.drop(columns=['Unnamed: 0','text_ocr'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:48.467285Z","iopub.execute_input":"2025-02-09T16:46:48.467562Z","iopub.status.idle":"2025-02-09T16:46:48.485991Z","shell.execute_reply.started":"2025-02-09T16:46:48.467532Z","shell.execute_reply":"2025-02-09T16:46:48.485055Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df = df.dropna()\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:48.487019Z","iopub.execute_input":"2025-02-09T16:46:48.487323Z","iopub.status.idle":"2025-02-09T16:46:48.510511Z","shell.execute_reply.started":"2025-02-09T16:46:48.487295Z","shell.execute_reply":"2025-02-09T16:46:48.509553Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"     image_name                                     text_corrected  \\\n0   image_1.jpg  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   \n1  image_2.jpeg  The best of #10 YearChallenge! Completed in le...   \n2   image_3.JPG  Sam Thorne @Strippin ( Follow Follow Saw every...   \n3   image_4.png              10 Year Challenge - Sweet Dee Edition   \n4   image_5.png  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   \n\n       humour          sarcasm       offensive      motivational  \\\n0   hilarious          general   not_offensive  not_motivational   \n1   not_funny          general   not_offensive      motivational   \n2  very_funny    not_sarcastic   not_offensive  not_motivational   \n3  very_funny  twisted_meaning  very_offensive      motivational   \n4   hilarious     very_twisted  very_offensive  not_motivational   \n\n  overall_sentiment  \n0     very_positive  \n1     very_positive  \n2          positive  \n3          positive  \n4           neutral  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>text_corrected</th>\n      <th>humour</th>\n      <th>sarcasm</th>\n      <th>offensive</th>\n      <th>motivational</th>\n      <th>overall_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>image_1.jpg</td>\n      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n      <td>hilarious</td>\n      <td>general</td>\n      <td>not_offensive</td>\n      <td>not_motivational</td>\n      <td>very_positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>image_2.jpeg</td>\n      <td>The best of #10 YearChallenge! Completed in le...</td>\n      <td>not_funny</td>\n      <td>general</td>\n      <td>not_offensive</td>\n      <td>motivational</td>\n      <td>very_positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>image_3.JPG</td>\n      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n      <td>very_funny</td>\n      <td>not_sarcastic</td>\n      <td>not_offensive</td>\n      <td>not_motivational</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>image_4.png</td>\n      <td>10 Year Challenge - Sweet Dee Edition</td>\n      <td>very_funny</td>\n      <td>twisted_meaning</td>\n      <td>very_offensive</td>\n      <td>motivational</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>image_5.png</td>\n      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n      <td>hilarious</td>\n      <td>very_twisted</td>\n      <td>very_offensive</td>\n      <td>not_motivational</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import re\n\ndef clean_text(text):\n    # Remove @usernames\n    text = re.sub(r'@\\w+', '', text)\n    \n    # Remove emails\n    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Remove all symbols except spaces and alphanumeric characters\n    text = re.sub(r'[^a-z0-9\\s]', '', text)\n    \n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:48.513940Z","iopub.execute_input":"2025-02-09T16:46:48.514232Z","iopub.status.idle":"2025-02-09T16:46:48.519426Z","shell.execute_reply.started":"2025-02-09T16:46:48.514203Z","shell.execute_reply":"2025-02-09T16:46:48.518545Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df2 = df.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:48.521362Z","iopub.execute_input":"2025-02-09T16:46:48.521553Z","iopub.status.idle":"2025-02-09T16:46:48.531406Z","shell.execute_reply.started":"2025-02-09T16:46:48.521537Z","shell.execute_reply":"2025-02-09T16:46:48.530560Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"df2['text_corrected'] = df2['text_corrected'].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:48.532278Z","iopub.execute_input":"2025-02-09T16:46:48.532593Z","iopub.status.idle":"2025-02-09T16:46:48.632240Z","shell.execute_reply.started":"2025-02-09T16:46:48.532569Z","shell.execute_reply":"2025-02-09T16:46:48.631451Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"df2.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:48.633034Z","iopub.execute_input":"2025-02-09T16:46:48.633264Z","iopub.status.idle":"2025-02-09T16:46:48.642386Z","shell.execute_reply.started":"2025-02-09T16:46:48.633234Z","shell.execute_reply":"2025-02-09T16:46:48.641521Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"     image_name                                     text_corrected  \\\n0   image_1.jpg  look there my friend lightyear now all sohalik...   \n1  image_2.jpeg  the best of 10 yearchallenge completed in less...   \n2   image_3.JPG  sam thorne follow follow saw everyone posting ...   \n3   image_4.png                10 year challenge sweet dee edition   \n4   image_5.png  10 year challenge with no filter 47 hilarious ...   \n\n       humour          sarcasm       offensive      motivational  \\\n0   hilarious          general   not_offensive  not_motivational   \n1   not_funny          general   not_offensive      motivational   \n2  very_funny    not_sarcastic   not_offensive  not_motivational   \n3  very_funny  twisted_meaning  very_offensive      motivational   \n4   hilarious     very_twisted  very_offensive  not_motivational   \n\n  overall_sentiment  \n0     very_positive  \n1     very_positive  \n2          positive  \n3          positive  \n4           neutral  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>text_corrected</th>\n      <th>humour</th>\n      <th>sarcasm</th>\n      <th>offensive</th>\n      <th>motivational</th>\n      <th>overall_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>image_1.jpg</td>\n      <td>look there my friend lightyear now all sohalik...</td>\n      <td>hilarious</td>\n      <td>general</td>\n      <td>not_offensive</td>\n      <td>not_motivational</td>\n      <td>very_positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>image_2.jpeg</td>\n      <td>the best of 10 yearchallenge completed in less...</td>\n      <td>not_funny</td>\n      <td>general</td>\n      <td>not_offensive</td>\n      <td>motivational</td>\n      <td>very_positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>image_3.JPG</td>\n      <td>sam thorne follow follow saw everyone posting ...</td>\n      <td>very_funny</td>\n      <td>not_sarcastic</td>\n      <td>not_offensive</td>\n      <td>not_motivational</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>image_4.png</td>\n      <td>10 year challenge sweet dee edition</td>\n      <td>very_funny</td>\n      <td>twisted_meaning</td>\n      <td>very_offensive</td>\n      <td>motivational</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>image_5.png</td>\n      <td>10 year challenge with no filter 47 hilarious ...</td>\n      <td>hilarious</td>\n      <td>very_twisted</td>\n      <td>very_offensive</td>\n      <td>not_motivational</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:48.643329Z","iopub.execute_input":"2025-02-09T16:46:48.643613Z","iopub.status.idle":"2025-02-09T16:46:49.695965Z","shell.execute_reply.started":"2025-02-09T16:46:48.643581Z","shell.execute_reply":"2025-02-09T16:46:49.695121Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"def remove_stopwords(text, custom_stopwords=None):\n    # Load default stopwords\n    stop_words = set(stopwords.words('english'))\n    stop_words.update({'follow'})\n\n    # Remove stopwords\n    filtered_text = ' '.join(word for word in text.split() if word not in stop_words)\n    \n    return filtered_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:49.696829Z","iopub.execute_input":"2025-02-09T16:46:49.697193Z","iopub.status.idle":"2025-02-09T16:46:49.701316Z","shell.execute_reply.started":"2025-02-09T16:46:49.697164Z","shell.execute_reply":"2025-02-09T16:46:49.700467Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"df2['text_corrected'] = df2['text_corrected'].apply(remove_stopwords)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:49.702114Z","iopub.execute_input":"2025-02-09T16:46:49.702343Z","iopub.status.idle":"2025-02-09T16:46:50.441305Z","shell.execute_reply.started":"2025-02-09T16:46:49.702312Z","shell.execute_reply":"2025-02-09T16:46:50.440668Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"df2['overall_sentiment'] = df2['overall_sentiment'].replace({'very_positive': 2, 'positive': 2, 'negative': 0, 'very_negative': 0, 'neutral': 1})\ndf2['overall_sentiment'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:50.442227Z","iopub.execute_input":"2025-02-09T16:46:50.442515Z","iopub.status.idle":"2025-02-09T16:46:50.458759Z","shell.execute_reply.started":"2025-02-09T16:46:50.442493Z","shell.execute_reply":"2025-02-09T16:46:50.457976Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-16-86f3814db7b8>:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df2['overall_sentiment'] = df2['overall_sentiment'].replace({'very_positive': 2, 'positive': 2, 'negative': 0, 'very_negative': 0, 'neutral': 1})\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"overall_sentiment\n2    4156\n1    2200\n0     631\nName: count, dtype: int64"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"df2.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:50.459547Z","iopub.execute_input":"2025-02-09T16:46:50.459832Z","iopub.status.idle":"2025-02-09T16:46:50.468937Z","shell.execute_reply.started":"2025-02-09T16:46:50.459813Z","shell.execute_reply":"2025-02-09T16:46:50.468009Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"     image_name                                     text_corrected  \\\n0   image_1.jpg  look friend lightyear sohalikut trend play 10 ...   \n1  image_2.jpeg  best 10 yearchallenge completed less 4 years k...   \n2   image_3.JPG  sam thorne saw everyone posting 2009 vs 2019 p...   \n3   image_4.png                10 year challenge sweet dee edition   \n4   image_5.png  10 year challenge filter 47 hilarious 10 year ...   \n\n       humour          sarcasm       offensive      motivational  \\\n0   hilarious          general   not_offensive  not_motivational   \n1   not_funny          general   not_offensive      motivational   \n2  very_funny    not_sarcastic   not_offensive  not_motivational   \n3  very_funny  twisted_meaning  very_offensive      motivational   \n4   hilarious     very_twisted  very_offensive  not_motivational   \n\n   overall_sentiment  \n0                  2  \n1                  2  \n2                  2  \n3                  2  \n4                  1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>text_corrected</th>\n      <th>humour</th>\n      <th>sarcasm</th>\n      <th>offensive</th>\n      <th>motivational</th>\n      <th>overall_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>image_1.jpg</td>\n      <td>look friend lightyear sohalikut trend play 10 ...</td>\n      <td>hilarious</td>\n      <td>general</td>\n      <td>not_offensive</td>\n      <td>not_motivational</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>image_2.jpeg</td>\n      <td>best 10 yearchallenge completed less 4 years k...</td>\n      <td>not_funny</td>\n      <td>general</td>\n      <td>not_offensive</td>\n      <td>motivational</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>image_3.JPG</td>\n      <td>sam thorne saw everyone posting 2009 vs 2019 p...</td>\n      <td>very_funny</td>\n      <td>not_sarcastic</td>\n      <td>not_offensive</td>\n      <td>not_motivational</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>image_4.png</td>\n      <td>10 year challenge sweet dee edition</td>\n      <td>very_funny</td>\n      <td>twisted_meaning</td>\n      <td>very_offensive</td>\n      <td>motivational</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>image_5.png</td>\n      <td>10 year challenge filter 47 hilarious 10 year ...</td>\n      <td>hilarious</td>\n      <td>very_twisted</td>\n      <td>very_offensive</td>\n      <td>not_motivational</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertTokenizer, DistilBertModel\nfrom transformers import ViTImageProcessor, ViTModel\nfrom PIL import Image, ImageFile\nimport pandas as pd\nfrom sklearn.metrics import f1_score\nfrom sklearn.decomposition import PCA\nimport torch.optim as optim\nimport numpy as np\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:46:50.469691Z","iopub.execute_input":"2025-02-09T16:46:50.470048Z","iopub.status.idle":"2025-02-09T16:47:08.782650Z","shell.execute_reply.started":"2025-02-09T16:46:50.470020Z","shell.execute_reply":"2025-02-09T16:47:08.781949Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Configuration\nTEXT_MODEL_NAME = 'distilbert-base-uncased'\nIMAGE_MODEL_NAME = 'google/vit-base-patch16-224'\nMAX_LENGTH = 128\nBATCH_SIZE = 32\nEPOCHS = 10\nLEARNING_RATE = 2e-5\nPCA_COMPONENTS = 256  # Reduce feature dimensions\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:47:08.783440Z","iopub.execute_input":"2025-02-09T16:47:08.783892Z","iopub.status.idle":"2025-02-09T16:47:08.854881Z","shell.execute_reply.started":"2025-02-09T16:47:08.783870Z","shell.execute_reply":"2025-02-09T16:47:08.853921Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Initialize processors\ntext_tokenizer = DistilBertTokenizer.from_pretrained(TEXT_MODEL_NAME)\nimage_processor = ViTImageProcessor.from_pretrained(IMAGE_MODEL_NAME)\ntext_model = DistilBertModel.from_pretrained(TEXT_MODEL_NAME).to(DEVICE)\nimage_model = ViTModel.from_pretrained(IMAGE_MODEL_NAME).to(DEVICE)\n\n# Freeze transformers to avoid recomputation\ntext_model.eval()\nimage_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:47:08.855898Z","iopub.execute_input":"2025-02-09T16:47:08.856300Z","iopub.status.idle":"2025-02-09T16:47:13.800448Z","shell.execute_reply.started":"2025-02-09T16:47:08.856266Z","shell.execute_reply":"2025-02-09T16:47:13.799767Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ea550e4f72c4d318c7575575852ebbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea5c4e8f0c7c403fa5f3e9142ecf6458"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ee11700b1a741ce80324fd0593eec6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e067535e2b940d6ac02db4f1a52dcce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b68b3c97b0940aca8d1ab3c0efabe3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e50b8167f884020a29a57509e02c8a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec524edcb3d64ed58290498d06d29dfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4a5a5ef45724e74b28c0683478fc6a3"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"ViTModel(\n  (embeddings): ViTEmbeddings(\n    (patch_embeddings): ViTPatchEmbeddings(\n      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (dropout): Dropout(p=0.0, inplace=False)\n  )\n  (encoder): ViTEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x ViTLayer(\n        (attention): ViTSdpaAttention(\n          (attention): ViTSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  (pooler): ViTPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"def preprocess_data(dataframe):\n    texts = dataframe['text_corrected'].tolist()\n    image_dir = \"/kaggle/input/multimodal-sentiment/Multimodal_dataset_assignment3/images\"\n    image_paths = dataframe['image_name'].apply(lambda name: os.path.join(image_dir, name))\n    # image_paths = dataframe['image_path'].tolist()\n    labels = torch.tensor(dataframe[['overall_sentiment']].values, dtype=torch.float)\n    \n    text_features, image_features = [], []\n    \n    with torch.no_grad():\n        for text, image_path in zip(texts, image_paths):\n            text_inputs = text_tokenizer(text, max_length=MAX_LENGTH, padding='max_length', truncation=True, return_tensors='pt').to(DEVICE)\n            text_output = text_model(**text_inputs).last_hidden_state[:, 0, :].cpu().numpy()\n\n            ImageFile.LOAD_TRUNCATED_IMAGES = True\n            image = Image.open(image_path).convert('RGB')\n            image_inputs = image_processor(images=image, return_tensors='pt').to(DEVICE)\n            image_output = image_model(**image_inputs).last_hidden_state[:, 0, :].cpu().numpy()\n            \n            text_features.append(text_output.squeeze())\n            image_features.append(image_output.squeeze())\n    \n    return np.array(text_features), np.array(image_features), labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:47:13.801271Z","iopub.execute_input":"2025-02-09T16:47:13.801517Z","iopub.status.idle":"2025-02-09T16:47:13.807703Z","shell.execute_reply.started":"2025-02-09T16:47:13.801480Z","shell.execute_reply":"2025-02-09T16:47:13.806760Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class MemeDataset(Dataset):\n    def __init__(self, text_features, image_features, labels):\n        self.text_features = text_features\n        self.image_features = image_features\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.text_features)\n\n    def __getitem__(self, idx):\n        return {\n            'text_features': torch.tensor(self.text_features[idx], dtype=torch.float),\n            'image_features': torch.tensor(self.image_features[idx], dtype=torch.float),\n            'labels': self.labels[idx]\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:47:13.808516Z","iopub.execute_input":"2025-02-09T16:47:13.808807Z","iopub.status.idle":"2025-02-09T16:47:13.938634Z","shell.execute_reply.started":"2025-02-09T16:47:13.808779Z","shell.execute_reply":"2025-02-09T16:47:13.937718Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class MultimodalModel(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.fc_overall = nn.Linear(input_dim, 3)\n        self.dropout = nn.Dropout(0.1)\n        # self.sigmoid = nn.Sigmoid()\n\n    def forward(self, text_features, image_features):\n        combined = torch.cat([text_features, image_features], dim=1)\n        combined = self.dropout(combined)\n        \n        # combined = self.sigmoid(self.fc_overall(combined))\n        logits = self.fc_overall(combined)\n        \n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:47:13.939452Z","iopub.execute_input":"2025-02-09T16:47:13.939646Z","iopub.status.idle":"2025-02-09T16:47:13.952466Z","shell.execute_reply.started":"2025-02-09T16:47:13.939629Z","shell.execute_reply":"2025-02-09T16:47:13.951624Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def evaluate(model, loader, device):\n    model.eval()\n    overall_preds, overall_labels = [], []\n    \n    with torch.no_grad():\n        for batch in loader:\n            text_features = batch['text_features'].to(device)\n            image_features = batch['image_features'].to(device)\n            labels = batch['labels'].cpu().numpy()\n            # labels = labels + 1  # Shift labels to range [0, 1, 2]\n            \n            outputs = model(text_features, image_features).cpu().numpy()\n            preds = outputs.argmax(axis=1)  # Use `axis=1` for NumPy array\n            # preds_final = [p - 1 for p in preds]  # Convert (0,1,2) -> (-1,0,1)\n            \n            overall_preds.extend(preds)\n            \n            overall_labels.extend(labels)\n            # print(preds,\"   \",labels)\n    metrics = {\n        'overall_f1': f1_score(overall_labels, overall_preds, average='macro')\n    }\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:47:13.956043Z","iopub.execute_input":"2025-02-09T16:47:13.956292Z","iopub.status.idle":"2025-02-09T16:47:13.963248Z","shell.execute_reply.started":"2025-02-09T16:47:13.956264Z","shell.execute_reply":"2025-02-09T16:47:13.962515Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split dataset\ntrain_df, test_df = train_test_split(df2, test_size=0.2, random_state=42)\n\ntrain_text, train_image, train_labels = preprocess_data(train_df)\nval_text, test_image, test_labels = preprocess_data(test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:47:13.964358Z","iopub.execute_input":"2025-02-09T16:47:13.964553Z","iopub.status.idle":"2025-02-09T16:51:16.342768Z","shell.execute_reply.started":"2025-02-09T16:47:13.964536Z","shell.execute_reply":"2025-02-09T16:51:16.342001Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"pca = PCA(n_components=PCA_COMPONENTS)\ntrain_text = pca.fit_transform(train_text)\ntrain_image = pca.fit_transform(train_image)\nval_image = pca.transform(test_image)\nval_text = pca.transform(val_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:51:16.343796Z","iopub.execute_input":"2025-02-09T16:51:16.344141Z","iopub.status.idle":"2025-02-09T16:51:17.118537Z","shell.execute_reply.started":"2025-02-09T16:51:16.344109Z","shell.execute_reply":"2025-02-09T16:51:17.117552Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = MemeDataset(train_text, train_image, train_labels)\nval_dataset = MemeDataset(val_text, val_image, test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:51:17.119450Z","iopub.execute_input":"2025-02-09T16:51:17.119702Z","iopub.status.idle":"2025-02-09T16:51:17.123544Z","shell.execute_reply.started":"2025-02-09T16:51:17.119681Z","shell.execute_reply":"2025-02-09T16:51:17.122801Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:51:17.124359Z","iopub.execute_input":"2025-02-09T16:51:17.124641Z","iopub.status.idle":"2025-02-09T16:51:17.138184Z","shell.execute_reply.started":"2025-02-09T16:51:17.124613Z","shell.execute_reply":"2025-02-09T16:51:17.137565Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"model = MultimodalModel(input_dim=PCA_COMPONENTS * 2).to(DEVICE)\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\ncriterion = torch.nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:52:36.171319Z","iopub.execute_input":"2025-02-09T16:52:36.171622Z","iopub.status.idle":"2025-02-09T16:52:36.176987Z","shell.execute_reply.started":"2025-02-09T16:52:36.171598Z","shell.execute_reply":"2025-02-09T16:52:36.176192Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"model.train()\nfor epoch in range(EPOCHS):\n    total_loss = 0\n    for batch in train_loader:\n            optimizer.zero_grad()\n            \n            text_features = batch['text_features'].to(DEVICE)\n            image_features = batch['image_features'].to(DEVICE)\n            labels = batch[\"labels\"].squeeze().long().to(DEVICE)\n\n            outputs = model(text_features, image_features)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n        \n    val_metrics = evaluate(model, val_loader, DEVICE)\n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n    print(f\"Train Loss: {total_loss/len(train_loader):.4f}\")\n    print(f\"Val F1 Scores - Overall: {val_metrics['overall_f1']:.4f}\")\nval_metrics = evaluate(model, val_loader, DEVICE)\ncheckpoint = {\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': loss,\n                'val_f1': val_metrics['overall_f1']\n            }\n\ntorch.save(checkpoint, 'checkpoint_TaskA.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:52:37.809027Z","iopub.execute_input":"2025-02-09T16:52:37.809319Z","iopub.status.idle":"2025-02-09T16:52:41.612634Z","shell.execute_reply.started":"2025-02-09T16:52:37.809295Z","shell.execute_reply":"2025-02-09T16:52:41.611800Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\nTrain Loss: 1.2106\nVal F1 Scores - Overall: 0.3118\nEpoch 2/10\nTrain Loss: 1.1934\nVal F1 Scores - Overall: 0.3106\nEpoch 3/10\nTrain Loss: 1.1863\nVal F1 Scores - Overall: 0.3111\nEpoch 4/10\nTrain Loss: 1.1799\nVal F1 Scores - Overall: 0.3116\nEpoch 5/10\nTrain Loss: 1.1739\nVal F1 Scores - Overall: 0.3116\nEpoch 6/10\nTrain Loss: 1.1670\nVal F1 Scores - Overall: 0.3105\nEpoch 7/10\nTrain Loss: 1.1614\nVal F1 Scores - Overall: 0.3099\nEpoch 8/10\nTrain Loss: 1.1556\nVal F1 Scores - Overall: 0.3087\nEpoch 9/10\nTrain Loss: 1.1503\nVal F1 Scores - Overall: 0.3080\nEpoch 10/10\nTrain Loss: 1.1447\nVal F1 Scores - Overall: 0.3078\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"checkpoint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T16:52:49.373118Z","iopub.execute_input":"2025-02-09T16:52:49.373427Z","iopub.status.idle":"2025-02-09T16:52:49.388559Z","shell.execute_reply.started":"2025-02-09T16:52:49.373402Z","shell.execute_reply":"2025-02-09T16:52:49.387707Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"{'epoch': 9,\n 'model_state_dict': OrderedDict([('fc_overall.weight',\n               tensor([[ 0.0254, -0.0060,  0.0123,  ...,  0.0030,  0.0001,  0.0063],\n                       [ 0.0286,  0.0020,  0.0242,  ..., -0.0170,  0.0316, -0.0157],\n                       [ 0.0176, -0.0369,  0.0126,  ...,  0.0065, -0.0084, -0.0086]],\n                      device='cuda:0')),\n              ('fc_overall.bias',\n               tensor([-0.0343,  0.0245,  0.0006], device='cuda:0'))]),\n 'optimizer_state_dict': {'state': {0: {'step': tensor(1750.),\n    'exp_avg': tensor([[-6.2981e-02, -2.7742e-03,  7.0067e-05,  ...,  9.9128e-03,\n             -2.5723e-03, -1.0763e-02],\n            [ 2.4255e-02, -2.2533e-02,  9.4727e-03,  ...,  1.1019e-02,\n             -8.8242e-03,  9.5642e-03],\n            [ 3.8725e-02,  2.5307e-02, -9.5427e-03,  ..., -2.0932e-02,\n              1.1397e-02,  1.1987e-03]], device='cuda:0'),\n    'exp_avg_sq': tensor([[0.0131, 0.0042, 0.0026,  ..., 0.0015, 0.0014, 0.0015],\n            [0.0204, 0.0063, 0.0039,  ..., 0.0021, 0.0020, 0.0022],\n            [0.0272, 0.0093, 0.0054,  ..., 0.0031, 0.0031, 0.0030]],\n           device='cuda:0')},\n   1: {'step': tensor(1750.),\n    'exp_avg': tensor([ 0.2124,  0.0205, -0.2329], device='cuda:0'),\n    'exp_avg_sq': tensor([0.0480, 0.0066, 0.0648], device='cuda:0')}},\n  'param_groups': [{'lr': 2e-05,\n    'betas': (0.9, 0.999),\n    'eps': 1e-08,\n    'weight_decay': 0.01,\n    'amsgrad': False,\n    'foreach': None,\n    'maximize': False,\n    'capturable': False,\n    'differentiable': False,\n    'fused': None,\n    'params': [0, 1]}]},\n 'loss': tensor(1.1197, device='cuda:0', grad_fn=<NllLossBackward0>),\n 'val_f1': 0.30777666471213955}"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}